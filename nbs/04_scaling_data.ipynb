{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cudf\n",
    "import numba\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "# parallelize the process on all columns using joblib\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Union, Optional, Dict, Any\n",
    "\n",
    "from signalslite.data_utils import (\n",
    "    load_recent_data_from_file,\n",
    "    save_daily_data,\n",
    "    save_in_folders,\n",
    "    get_latest_date,\n",
    "    read_available_dates\n",
    ")\n",
    "from signalslite.constants import Directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_cut(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    # NOTE: Parallel is a library for parallelizing operations\n",
    "    # on a single machine. It is not a distributed framework.\n",
    "    # The `n_jobs` argument specifies how many CPU cores to use.\n",
    "    # For example, `n_jobs=10` will use 10 CPU cores.\n",
    "    _res = Parallel(\n",
    "        n_jobs=10,\n",
    "    )(delayed(pd.qcut)(df[col], q=5, labels=False, duplicates=\"drop\") for col in cols)\n",
    "    _res = pd.concat(_res, axis=1).astype(\"int8\")\n",
    "    return _res\n",
    "\n",
    "def apply_cut_cpu(recent_data: pd.DataFrame, feature_columns: List[str]) -> pd.DataFrame:\n",
    "    recent_data = recent_data.dropna(subset=feature_columns, axis=0)\n",
    "    recent_data_gpu = cudf.DataFrame.from_pandas(recent_data)\n",
    "\n",
    "    ranks = (\n",
    "        recent_data_gpu[[\"date\"] + feature_columns]\n",
    "        .groupby(\"date\")\n",
    "        .rank(pct=True, method=\"first\", ascending=True, na_option=\"keep\")\n",
    "    )\n",
    "    ranks[\"close\"] = recent_data[\"close\"]\n",
    "    ranks[\"date\"] = recent_data[\"date\"]\n",
    "    ranks[\"bloomberg_ticker\"] = recent_data[\"bloomberg_ticker\"]\n",
    "\n",
    "    ranks_pd = ranks.to_pandas()\n",
    "\n",
    "    del recent_data_gpu, ranks\n",
    "\n",
    "    # NOTE: This code will be run on the CPU.\n",
    "    # We will show you how to run this code on the GPU in the next section.\n",
    "    res = ranks_pd.groupby(\"date\").apply(lambda df: apply_cut(df, feature_columns))\n",
    "    res[\"date\"] = ranks_pd[\"date\"]\n",
    "    res[\"bloomberg_ticker\"] = ranks_pd[\"bloomberg_ticker\"]\n",
    "    res[\"close\"] = ranks_pd[\"close\"]\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this code is used to apply a cut on the data, in order to remove some of the data that has been corrupted\n",
    "# the cut is based on the standard deviation of the data, and is applied on a per-feature basis\n",
    "# it is applied to the data in chunks of 200 days, in order to avoid memory issues\n",
    "def scale_data(dir_config, FROM_SCRATCH=False):\n",
    "    dates = read_available_dates(dir_config.DAILY_SECONDARY_FEATURES_DIR)\n",
    "\n",
    "    start_index = len(dates) - 1000 if not FROM_SCRATCH else 0\n",
    "\n",
    "    # iterate over all dates in chunks of 200\n",
    "    for i in tqdm(range(start_index, len(dates), 200)):\n",
    "        _tmp = load_recent_data_from_file(\n",
    "            dir_config.DAILY_SECONDARY_FEATURES_DIR,\n",
    "            n_days=200,\n",
    "            ascending=True,\n",
    "            offset=i,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "        _tmp = _tmp.reset_index(drop=True)\n",
    "        _tmp = _tmp.sort_values([\"date\", \"bloomberg_ticker\"])\n",
    "        _tmp = _tmp.groupby(\"date\").filter(lambda x: len(x) > 10)\n",
    "\n",
    "        # get all feature columns\n",
    "        feature_columns = [f for f in _tmp.columns if f.startswith(\"feature_\")]\n",
    "\n",
    "        # apply cut\n",
    "        res = apply_cut_cpu(_tmp, feature_columns)\n",
    "\n",
    "        # save\n",
    "        save_in_folders(res, dir_config.DAILY_SCALED_FEATURES_DIR)\n",
    "\n",
    "        del _tmp, res\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feature_1_sma_5', 'feature_1_sma_10', 'feature_1_sma_20',\n",
      "       'feature_1_sma_50', 'feature_1_sma_100', 'feature_1_sma_200',\n",
      "       'feature_1_ema_5', 'feature_1_ema_10', 'feature_1_ema_20',\n",
      "       'feature_1_ema_50',\n",
      "       ...\n",
      "       'feature_2_ratio_rsi_100_200', 'feature_2_ratio_rsi_5_close',\n",
      "       'feature_2_ratio_rsi_10_close', 'feature_2_ratio_rsi_20_close',\n",
      "       'feature_2_ratio_rsi_50_close', 'feature_2_ratio_rsi_100_close',\n",
      "       'feature_2_ratio_rsi_200_close', 'close', 'date', 'bloomberg_ticker'],\n",
      "      dtype='object', length=104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6359/3164898052.py:33: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  res = ranks_pd.groupby(\"date\").apply(lambda df: apply_cut(df, feature_columns))\n",
      "100%|██████████| 200/200 [00:00<00:00, 369.22it/s]\n",
      " 20%|██        | 1/5 [00:14<00:56, 14.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feature_1_sma_5', 'feature_1_sma_10', 'feature_1_sma_20',\n",
      "       'feature_1_sma_50', 'feature_1_sma_100', 'feature_1_sma_200',\n",
      "       'feature_1_ema_5', 'feature_1_ema_10', 'feature_1_ema_20',\n",
      "       'feature_1_ema_50',\n",
      "       ...\n",
      "       'feature_2_ratio_rsi_100_200', 'feature_2_ratio_rsi_5_close',\n",
      "       'feature_2_ratio_rsi_10_close', 'feature_2_ratio_rsi_20_close',\n",
      "       'feature_2_ratio_rsi_50_close', 'feature_2_ratio_rsi_100_close',\n",
      "       'feature_2_ratio_rsi_200_close', 'close', 'date', 'bloomberg_ticker'],\n",
      "      dtype='object', length=104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6359/3164898052.py:33: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  res = ranks_pd.groupby(\"date\").apply(lambda df: apply_cut(df, feature_columns))\n",
      "100%|██████████| 199/199 [00:00<00:00, 746.56it/s]\n",
      " 40%|████      | 2/5 [00:30<00:46, 15.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feature_1_sma_5', 'feature_1_sma_10', 'feature_1_sma_20',\n",
      "       'feature_1_sma_50', 'feature_1_sma_100', 'feature_1_sma_200',\n",
      "       'feature_1_ema_5', 'feature_1_ema_10', 'feature_1_ema_20',\n",
      "       'feature_1_ema_50',\n",
      "       ...\n",
      "       'feature_2_ratio_rsi_100_200', 'feature_2_ratio_rsi_5_close',\n",
      "       'feature_2_ratio_rsi_10_close', 'feature_2_ratio_rsi_20_close',\n",
      "       'feature_2_ratio_rsi_50_close', 'feature_2_ratio_rsi_100_close',\n",
      "       'feature_2_ratio_rsi_200_close', 'close', 'date', 'bloomberg_ticker'],\n",
      "      dtype='object', length=104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6359/3164898052.py:33: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  res = ranks_pd.groupby(\"date\").apply(lambda df: apply_cut(df, feature_columns))\n",
      "100%|██████████| 200/200 [00:00<00:00, 727.95it/s]\n",
      " 60%|██████    | 3/5 [00:47<00:32, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feature_1_sma_5', 'feature_1_sma_10', 'feature_1_sma_20',\n",
      "       'feature_1_sma_50', 'feature_1_sma_100', 'feature_1_sma_200',\n",
      "       'feature_1_ema_5', 'feature_1_ema_10', 'feature_1_ema_20',\n",
      "       'feature_1_ema_50',\n",
      "       ...\n",
      "       'feature_2_ratio_rsi_100_200', 'feature_2_ratio_rsi_5_close',\n",
      "       'feature_2_ratio_rsi_10_close', 'feature_2_ratio_rsi_20_close',\n",
      "       'feature_2_ratio_rsi_50_close', 'feature_2_ratio_rsi_100_close',\n",
      "       'feature_2_ratio_rsi_200_close', 'close', 'date', 'bloomberg_ticker'],\n",
      "      dtype='object', length=104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6359/3164898052.py:33: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  res = ranks_pd.groupby(\"date\").apply(lambda df: apply_cut(df, feature_columns))\n",
      "100%|██████████| 200/200 [00:00<00:00, 661.44it/s]\n",
      " 80%|████████  | 4/5 [01:06<00:17, 17.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['feature_1_sma_5', 'feature_1_sma_10', 'feature_1_sma_20',\n",
      "       'feature_1_sma_50', 'feature_1_sma_100', 'feature_1_sma_200',\n",
      "       'feature_1_ema_5', 'feature_1_ema_10', 'feature_1_ema_20',\n",
      "       'feature_1_ema_50',\n",
      "       ...\n",
      "       'feature_2_ratio_rsi_100_200', 'feature_2_ratio_rsi_5_close',\n",
      "       'feature_2_ratio_rsi_10_close', 'feature_2_ratio_rsi_20_close',\n",
      "       'feature_2_ratio_rsi_50_close', 'feature_2_ratio_rsi_100_close',\n",
      "       'feature_2_ratio_rsi_200_close', 'close', 'date', 'bloomberg_ticker'],\n",
      "      dtype='object', length=104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6359/3164898052.py:33: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  res = ranks_pd.groupby(\"date\").apply(lambda df: apply_cut(df, feature_columns))\n",
      "100%|██████████| 200/200 [00:00<00:00, 778.60it/s]\n",
      "100%|██████████| 5/5 [01:24<00:00, 16.83s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dir_config = Directories()\n",
    "    dir_config.set_data_dir(\"../data\")\n",
    "\n",
    "    FROM_SCRATCH = False\n",
    "\n",
    "    scale_data(dir_config, FROM_SCRATCH=FROM_SCRATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
